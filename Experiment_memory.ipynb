{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import SystemMessage, HumanMessage, ChatMessage\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, ChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history'], input_types={'chat_history': typing.List[typing.Union[langchain.schema.messages.AIMessage, langchain.schema.messages.HumanMessage, langchain.schema.messages.ChatMessage, langchain.schema.messages.SystemMessage, langchain.schema.messages.FunctionMessage]]}, messages=[SystemMessage(content='You are a smart AI'), MessagesPlaceholder(variable_name='chat_history')])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [SystemMessage(\n",
    "        content=\"You are a smart AI\"\n",
    "    ),\n",
    "     MessagesPlaceholder(\n",
    "         variable_name=\"chat_history\"\n",
    "     )]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a smart AI'),\n",
       " HumanMessage(content='Hi how are you?')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format_messages(chat_history=[HumanMessage(content=\"Hi how are you?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['latest_message'], template='{latest_message}'))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HumanMessagePromptTemplate.(\"{latest_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessage(content='Hello', role='User')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatMessage(role=\"User\", content=\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessagePromptTemplate(prompt=PromptTemplate(input_variables=['human_input'], template='{human_input}'), role='{name_input}')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatMessagePromptTemplate.from_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if not \"\":\n",
    "    print(\"True\")\n",
    "else:\n",
    "    print(\"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from discord.message import Message\n",
    "import os\n",
    "\n",
    "import src.config as CONFIG\n",
    "from src.big_query import database\n",
    "\n",
    "def convert_dict_to_msg_list(rows) -> list:\n",
    "    chat_hist = []\n",
    "    for row in rows:\n",
    "        msg_content = row[\"author_name\"] + \" said \\\"\" + row[\"message_content\"] + \"\\\"\"\n",
    "        if row[\"author_id\"] == \"1171714115069284373\":\n",
    "            chat_hist.append(msg_content)\n",
    "        else:\n",
    "            chat_hist.append(msg_content)\n",
    "    return chat_hist\n",
    "\n",
    "class SparksAI:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def load_core_mind_file(self) -> str:\n",
    "        file_path = CONFIG.MIND_FILE_PATH + \"\\core-file.md\"\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, \"r\") as file:\n",
    "                file_text = file.read()\n",
    "        else:\n",
    "            with open(CONFIG.MIND_FILE_PATH + \"core-file-starter.md\", \"r\") as file:\n",
    "                file_text = file.read()\n",
    "\n",
    "        return file_text\n",
    "\n",
    "    def self_reflect(self):\n",
    "        mind_file = self.load_core_mind_file()\n",
    "        all_msgs = convert_dict_to_msg_list(database.get_all_rows())\n",
    "\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "                                              {mind_file}\n",
    "                                              \n",
    "                                              Below is all chat messages you have sent and recieved:\n",
    "                                              \n",
    "                                              {all_msgs}\n",
    "                                              \n",
    "                                              Review the above messages and write a detailed analysis of kaimsparks. The analysis should include at least the following points but can include any additional information that is useful for future conversations:\n",
    "                                              \n",
    "                                              1. What do they want you to do most\n",
    "                                              \n",
    "                                              2. Main personality traits\n",
    "                                              \n",
    "                                              3. Likes and dislikes\n",
    "                                              \n",
    "                                              4. What are their main goals in life\n",
    "                                              \n",
    "                                              5. List of confirmed data and facts about kaimsparks\n",
    "\n",
    "                                              If you can not answer a section, add a placeholder stating more information is required.\n",
    "                                              \"\"\"\n",
    "        )\n",
    "\n",
    "        llm = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "\n",
    "        chain = prompt | llm\n",
    "\n",
    "        analysis = chain.invoke({\"mind_file\": mind_file, \"all_msgs\": all_msgs})\n",
    "\n",
    "        with open(\n",
    "            CONFIG.MIND_FILE_PATH + \"kaimsparks\" + \"-mind-file.md\",\n",
    "            \"w\",\n",
    "            encoding=\"utf-8\",\n",
    "        ) as file:\n",
    "            file.write(analysis.content)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def handle_message(self, msg: Message) -> None:\n",
    "        msg = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.big_query:Retrieved all rows\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "sparks = SparksAI()\n",
    "\n",
    "sparks.self_reflect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1\n",
      "hello\n",
      "test2\n",
      "goodbye\n"
     ]
    }
   ],
   "source": [
    "dict_files = {\"test1\": \"hello\", \"test2\": \"goodbye\"}\n",
    "\n",
    "for k, v in dict_files.items():\n",
    "    print(k)\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.mind.Mind at 0x248e26c4710>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.mind import Mind\n",
    "import src.config as CONFIG\n",
    "\n",
    "temp_mind = Mind(\n",
    "    path_to_mind_files=CONFIG.MIND_FILE_PATH,\n",
    "    path_to_starters=CONFIG.STARTER_FILE_PATH\n",
    ")\n",
    "\n",
    "temp_mind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_mind.save_mind_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
